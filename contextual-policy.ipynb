{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Contextual Bandit Problem\n",
    "\n",
    "## Reinforcement Learning approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The `ContextualBandit` (Slot machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class ContextualBandit():\n",
    "    \"\"\"\n",
    "    Contextual Bandit class.\n",
    "    \n",
    "    A three four-armed bandit is being used here. \n",
    "    What this means is that each bandit has four arms that can \n",
    "    be pulled. \n",
    "    \n",
    "    Each bandit has different success probabilities for \n",
    "    each arm, and as such requires different actions to \n",
    "    obtain the best result.\n",
    "    \n",
    "    The agent should learn how to always choose the \n",
    "    bandit-arm that will most often give a positive \n",
    "    reward, depending on the Bandit presented.\n",
    "    \n",
    "    -------- Attributes -------\n",
    "    :bandits\n",
    "    :states\n",
    "    :num_bandits\n",
    "    :num_actions\n",
    "    \n",
    "    -------- Methods --------\n",
    "    getBandit:\n",
    "        Returns a random state.\n",
    "    pullBandit:\n",
    "        Returns a reward for an action.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.bandits = np.array([[.2,  0, -0,  -5],\n",
    "                                 [.1, -5,  1, .25],\n",
    "                                 [-5,  5,  5,   5]])\n",
    "        self.state = 0\n",
    "        self.num_bandits = self.bandits.shape[0]\n",
    "        self.num_actions = self.bandits.shape[1]\n",
    "    \n",
    "    def getBandit(self):\n",
    "        \"\"\"Returns a random state\"\"\"\n",
    "        self.state = np.random.randint(0, len(self.bandits))\n",
    "        return self.state\n",
    "    \n",
    "    def pullBandit(self, action):\n",
    "        \"\"\"\n",
    "        Returns a reward for an action.\n",
    "        \n",
    "        A random number from a normal distribution with a mean of 0\n",
    "        is generated. The lower the bandit number, the more likely \n",
    "        a positive reward will be returned. \n",
    "        \"\"\"\n",
    "        bandit = self.bandits[self.state, action]\n",
    "        result = np.random.randn(1)\n",
    "        return 1 if result > bandit else -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### The `Agent` (Gambler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \"\"\"\n",
    "    The agent takes as input the current state, and returns an action. \n",
    "    \n",
    "    This allows the agent to take actions which are conditioned \n",
    "    on the state of the environment, a critical step toward being \n",
    "    able to solve full RL problems. \n",
    "    \n",
    "    The agent uses a single set of weights, within which each \n",
    "    value is an estimate of the value of the return from choosing \n",
    "    a particular arm given a bandit.\n",
    "    \n",
    "    A policy gradient method is used to update the agent \n",
    "    by moving the value for the selected action toward the \n",
    "    recieved reward.\n",
    "    \n",
    "    :param learning_rate: float\n",
    "    :param state_size: int\n",
    "    :param action_size: int\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate, state_size, action_size):\n",
    "        # inputs\n",
    "        with tf.variable_scope('agent') as scope:\n",
    "            self.state = tf.placeholder(tf.int32, shape=[1], name='state')\n",
    "            state_oh = tf.contrib.layers.one_hot_encoding(self.state, state_size)\n",
    "            # feed forward net\n",
    "            output = tf.contrib.layers.fully_connected(state_oh, action_size,\n",
    "                                                       activation_fn=tf.nn.sigmoid,\n",
    "                                                       weights_initializer=tf.ones_initializer(),\n",
    "                                                       biases_initializer=None, scope=scope)\n",
    "            # action & network output\n",
    "            self.output = tf.reshape(output, shape=[-1])\n",
    "            self.action = tf.argmax(self.output, axis=0, name='action')\n",
    "            # reward, action, weights\n",
    "            self.reward_holder = tf.placeholder(tf.float32, shape=[1], name='reward_holder')\n",
    "            self.action_holder = tf.placeholder(tf.int32,   shape=[1], name='action_holder')\n",
    "            self.weight = tf.slice(self.output, self.action_holder, size=[1], name='weight')\n",
    "            # loss function\n",
    "            loss = -(tf.log(self.weight) * self.reward_holder)\n",
    "            self.loss = tf.reduce_mean(loss, name='loss')\n",
    "            # Optimizer\n",
    "            self.global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "            self.trainer = optimizer.minimize(self.loss, global_step=self.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Environement and Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "bandit = ContextualBandit()\n",
    "agent = Agent(learning_rate=1e-2, \n",
    "              state_size=bandit.num_bandits, \n",
    "              action_size=bandit.num_actions)\n",
    "weights = tf.trainable_variables()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tensorflow's Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Restoring latest checkpoint\n",
      "INFO:tensorflow:Restoring parameters from saved/3-armed-bandits/models/model.ckpt-9901\n",
      "INFO: Successfully restored last checkpoint — saved/3-armed-bandits/models/model.ckpt-9901"
     ]
    }
   ],
   "source": [
    "env_name = f\"{bandit.num_bandits}-armed-bandit{'s' if bandit.num_bandits > 1 else ''}\"\n",
    "saved_dir = os.path.join('saved', env_name)\n",
    "tensorboard_dir = os.path.join(saved_dir, 'tensorboard')\n",
    "logdir = os.path.join(tensorboard_dir, 'log')\n",
    "\n",
    "tf.summary.scalar('agent/loss', agent.loss)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "model_dir = os.path.join(saved_dir, 'models')\n",
    "model_path = os.path.join(model_dir, 'model.ckpt')\n",
    "\n",
    "writer = tf.summary.FileWriter(logdir=logdir, graph=sess.graph)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "if tf.gfile.Exists(model_dir):\n",
    "    try:\n",
    "        sys.stdout.write(f'INFO: Restoring latest checkpoint\\n')\n",
    "        \n",
    "        last_ckpt = tf.train.latest_checkpoint(model_dir)\n",
    "        saver.restore(sess=sess, save_path=last_ckpt)\n",
    "\n",
    "        sys.stdout.write(f'INFO: Successfully restored last checkpoint — {last_ckpt}')\n",
    "        sys.stdout.flush()\n",
    "    except Exception as ex:\n",
    "        sys.stderr.write(f'ERR: Could not load checkpoint. {ex}')\n",
    "        sys.stderr.flush()\n",
    "else:\n",
    "    tf.gfile.MakeDirs(model_dir)\n",
    "    \n",
    "    sys.stdout.write(f'INFO: Checkpoint file does not exist.\\n'\n",
    "                     f'Creating checkpoint it {model_dir}')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "episodes = 10000\n",
    "save_interval = 100\n",
    "e = 0.1\n",
    "rewards = np.zeros(shape=[bandit.num_bandits, bandit.num_actions])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Training the `Agent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10,000\tGlobal step: 19,901\tMean reward: [ 764.5  753.   701.5]5]"
     ]
    }
   ],
   "source": [
    "for i in range(episodes):\n",
    "    state = bandit.getBandit()\n",
    "\n",
    "    if np.random.rand(1) < e:\n",
    "        action = np.random.randint(bandit.num_actions)\n",
    "    else:\n",
    "        action = sess.run(agent.action, feed_dict={agent.state: [state]})\n",
    "    \n",
    "    reward = bandit.pullBandit(action)\n",
    "    \n",
    "    feed_dict = {agent.reward_holder: [reward], \n",
    "                 agent.action_holder: [action],\n",
    "                 agent.state: [state]}\n",
    "    _, i_global, _weights = sess.run([agent.trainer, agent.global_step, weights], feed_dict=feed_dict)\n",
    "    \n",
    "    \n",
    "    if i%save_interval == 0:\n",
    "        saver.save(sess=sess, save_path=model_path, global_step=agent.global_step)\n",
    "        summary = sess.run(merged, feed_dict=feed_dict)\n",
    "        writer.add_summary(summary=summary, global_step=i_global)\n",
    "        \n",
    "    rewards[state, action] += reward\n",
    "    sys.stdout.write(f'\\rEpisode: {i+1:,}\\tGlobal step: {i_global:,}'\n",
    "                     f'\\tMean reward: {np.mean(rewards, axis=1)}')\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
